{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c873610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo de regresión logística: 61.51%\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.59      0.61      2073\n",
      "           1       0.60      0.64      0.62      1951\n",
      "\n",
      "    accuracy                           0.62      4024\n",
      "   macro avg       0.62      0.62      0.62      4024\n",
      "weighted avg       0.62      0.62      0.62      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar los datos CSV\n",
    "datos = pd.read_csv(\"fake-news/trainnuevo.csv\")\n",
    "\n",
    "# Convertir las etiquetas a tipo numérico\n",
    "datos['label'] = datos['label'].astype(int)\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = datos['text'].apply(len).values.reshape(-1, 1)  # Usar la longitud del texto como característica\n",
    "y = datos['label']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo de regresión logística: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Imprimir un informe de clasificación\n",
    "print(\"\\nInforme de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0c746af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo SVM: 61.33%\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.54      0.59      2073\n",
      "           1       0.59      0.69      0.64      1951\n",
      "\n",
      "    accuracy                           0.61      4024\n",
      "   macro avg       0.62      0.62      0.61      4024\n",
      "weighted avg       0.62      0.61      0.61      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar los datos CSV\n",
    "datos = pd.read_csv(\"fake-news/trainnuevo.csv\")\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = datos['text'].apply(len).values.reshape(-1, 1)  # Usar la longitud del texto como característica\n",
    "y = datos['label']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar un modelo SVM\n",
    "model = SVC(kernel='linear')  # Usar un kernel lineal para SVM\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo SVM: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Imprimir un informe de clasificación\n",
    "print(\"\\nInforme de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e1e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo Naive Bayes: 90.08%\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      2073\n",
      "           1       0.95      0.84      0.89      1951\n",
      "\n",
      "    accuracy                           0.90      4024\n",
      "   macro avg       0.91      0.90      0.90      4024\n",
      "weighted avg       0.91      0.90      0.90      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cargar los datos desde un archivo CSV\n",
    "datos = pd.read_csv(\"fake-news/trainnuevo.csv\")\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = datos['text']\n",
    "y = datos['label']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización de las características (con recuento de palabras)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Entrenar el modelo Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo Naive Bayes: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Imprimir un informe de clasificación\n",
    "print(\"\\nInforme de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b1e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo Naive Bayes: 90.08%\n",
      "\n",
      "Informe de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      2073\n",
      "           1       0.95      0.84      0.89      1951\n",
      "\n",
      "    accuracy                           0.90      4024\n",
      "   macro avg       0.91      0.90      0.90      4024\n",
      "weighted avg       0.91      0.90      0.90      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cargar los datos desde un archivo CSV\n",
    "datos = pd.read_csv(\"fake-news/trainnuevo.csv\")\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = datos['text']\n",
    "y = datos['label']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorización de las características (con recuento de palabras)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Entrenar el modelo Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Predecir las etiquetas para los datos de prueba\n",
    "y_pred = clf.predict(X_test_counts)\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Precisión del modelo Naive Bayes: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Imprimir un informe de clasificación\n",
    "print(\"\\nInforme de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210e2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.95\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      2073\n",
      "           1       0.95      0.94      0.94      1951\n",
      "\n",
      "    accuracy                           0.95      4024\n",
      "   macro avg       0.95      0.95      0.95      4024\n",
      "weighted avg       0.95      0.95      0.95      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('fake-news/trainnuevo.csv')  # Reemplaza 'datos_noticias.csv' con el nombre de tu archivo de datos\n",
    "\n",
    "# Paso 2: Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data['text']  # Columna que contiene el texto de las noticias\n",
    "y = data['label']  # Columna que contiene las etiquetas (0 para noticias verdaderas, 1 para noticias falsas)\n",
    "\n",
    "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 4: Vectorización de características (con TF-IDF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Vectorizador TF-IDF con un máximo de 5000 características\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Paso 5: Inicializar y entrenar el clasificador de Bosque Aleatorio\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Paso 6: Predecir las etiquetas en el conjunto de prueba\n",
    "y_pred = clf.predict(X_test_vectorized)\n",
    "\n",
    "# Paso 7: Evaluar el rendimiento del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {accuracy:.2f}')\n",
    "\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e94ad2e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "403/403 [==============================] - 21s 50ms/step - loss: 0.2864 - accuracy: 0.8632 - val_loss: 0.1527 - val_accuracy: 0.9369\n",
      "Epoch 2/5\n",
      "403/403 [==============================] - 21s 52ms/step - loss: 0.0779 - accuracy: 0.9737 - val_loss: 0.1438 - val_accuracy: 0.9469\n",
      "Epoch 3/5\n",
      "403/403 [==============================] - 23s 56ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.1617 - val_accuracy: 0.9484\n",
      "Epoch 4/5\n",
      "403/403 [==============================] - 23s 56ms/step - loss: 0.0038 - accuracy: 0.9995 - val_loss: 0.1641 - val_accuracy: 0.9534\n",
      "Epoch 5/5\n",
      "403/403 [==============================] - 22s 54ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1804 - val_accuracy: 0.9528\n",
      "126/126 [==============================] - 2s 14ms/step\n",
      "Precisión del modelo: 0.95\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      2073\n",
      "           1       0.95      0.95      0.95      1951\n",
      "\n",
      "    accuracy                           0.95      4024\n",
      "   macro avg       0.95      0.95      0.95      4024\n",
      "weighted avg       0.95      0.95      0.95      4024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Paso 1: Cargar los datos\n",
    "data = pd.read_csv('fake-news/trainnuevo.csv') \n",
    "\n",
    "# Paso 2: Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Paso 3: Convertir las etiquetas de texto a números\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Paso 4: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 5: Tokenización y secuenciación de texto\n",
    "max_words = 10000  # Número máximo de palabras a considerar\n",
    "max_len = 200  # Longitud máxima de la secuencia de entrada\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Paso 6: Construir el modelo de Red Neuronal Convolucional (CNN)\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Paso 7: Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Paso 8: Entrenar el modelo\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "model.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Paso 9: Evaluar el modelo\n",
    "\n",
    "y_pred_prob = model.predict(X_test_pad)\n",
    "y_pred = np.round(y_pred_prob).flatten().astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {accuracy:.2f}')\n",
    "\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56796887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "503/503 [==============================] - 40s 66ms/step - loss: 0.2956 - accuracy: 0.8736 - val_loss: 0.1742 - val_accuracy: 0.9312\n",
      "Epoch 2/10\n",
      "503/503 [==============================] - 27s 54ms/step - loss: 0.1209 - accuracy: 0.9553 - val_loss: 0.1762 - val_accuracy: 0.9312\n",
      "Epoch 3/10\n",
      "503/503 [==============================] - 27s 53ms/step - loss: 0.0717 - accuracy: 0.9775 - val_loss: 0.1971 - val_accuracy: 0.9327\n",
      "Epoch 4/10\n",
      "503/503 [==============================] - 27s 53ms/step - loss: 0.0372 - accuracy: 0.9882 - val_loss: 0.2575 - val_accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "503/503 [==============================] - 25s 50ms/step - loss: 0.0315 - accuracy: 0.9907 - val_loss: 0.2631 - val_accuracy: 0.9232\n",
      "Epoch 6/10\n",
      "503/503 [==============================] - 26s 53ms/step - loss: 0.0241 - accuracy: 0.9940 - val_loss: 0.2937 - val_accuracy: 0.9242\n",
      "Epoch 7/10\n",
      "503/503 [==============================] - 26s 52ms/step - loss: 0.0176 - accuracy: 0.9951 - val_loss: 0.3207 - val_accuracy: 0.9215\n",
      "Epoch 8/10\n",
      "503/503 [==============================] - 25s 50ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.3330 - val_accuracy: 0.9187\n",
      "Epoch 9/10\n",
      "503/503 [==============================] - 26s 53ms/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.3400 - val_accuracy: 0.9259\n",
      "Epoch 10/10\n",
      "503/503 [==============================] - 26s 51ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.4126 - val_accuracy: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa5041b5b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('fake-news/trainnuevo.csv')\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "max_words = 10000\n",
    "maxlen = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "y = data['label']\n",
    "\n",
    "# Construir el modelo de red neuronal\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef2b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
